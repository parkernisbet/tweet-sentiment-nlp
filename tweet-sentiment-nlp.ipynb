{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0039ba0ea65d4ae83123fad7f1d7f3994334a912b8446d9a8b7ab4243a3317da3",
   "display_name": "Python 3.8.8 64-bit ('tweet-sentiment-nlp': virtualenvwrapper)"
  },
  "metadata": {
   "interpreter": {
    "hash": "039ba0ea65d4ae83123fad7f1d7f3994334a912b8446d9a8b7ab4243a3317da3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Tweet Sentiment NLP"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This notebook covers sentiment analysis for a collection of 1.6 million tweets, sourced from this Kaggle [dataset](https://www.kaggle.com/kazanova/sentiment140). My goal is build a series of machine learning models that can predict document polarity reasonably accurately, with each attempt attacking the classification problem from a different natural language processing approach and level of sophistication. The below code can be split into three sections: generalized text preprocessing, single-layer model evaluation, and multi-layer model evaluation. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Generalized Text Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Basic Corpus Cleaning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "For the generalized text preprocessing section, modifications made are only those that do not intentionally \"lose\" tweet information (i.e. no dropping words or ther destructive changes). This then leaves only formatting, standardization, and other methods of purely data cleaning. See the \"general_preprocessing\" function for a full list and order of all data preparation techniques used. A general summary of changes made is as follows: replacing urls, usernames, emoticons, and unrecognized characters with words; expanding contractions and common abbreviations; removing all non-alphanumeric characters; and truncating egregiously long sequences of repeated characters. This should then leave the corpus comprised solely of recognized English language words and numbers."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if modules are installed\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "output = Popen(\"pip list | awk '{print $1}'\", shell = True, stdout=PIPE).stdout.read().split()\n",
    "packages = [x.decode('utf-8') for x in output][2:]\n",
    "modules = ['contractions', 'kaggle', 'nltk', 'pandas', 'scikit-learn']\n",
    "for nm in modules:\n",
    "    if nm not in packages:\n",
    "        ! pip install {nm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules and setup\n",
    "import contractions\n",
    "import csv\n",
    "import demoji\n",
    "import glob\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import zipfile\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading dataset\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "dataset = 'kazanova/sentiment140'\n",
    "csv_name = 'training.1600000.processed.noemoticon.csv'\n",
    "try:\n",
    "    os.remove(csv_name)\n",
    "except:\n",
    "    pass\n",
    "api.dataset_download_file(dataset, file_name=csv_name, path='./')\n",
    "fn = glob.glob('train*.zip', recursive = True)[0]\n",
    "with zipfile.ZipFile(fn) as zip_file:\n",
    "    for file in zip_file.namelist():\n",
    "        if file == csv_name:\n",
    "            zip_file.extract(csv_name)\n",
    "os.remove(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "df_data dimensions: (1600000, 2)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   target                                               text\n",
       "0       0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1       0  is upset that he can't update his Facebook by ...\n",
       "2       0  @Kenichan I dived many times for the ball. Man...\n",
       "3       0    my whole body feels itchy and like its on fire \n",
       "4       0  @nationwideclass no, it's not behaving at all...."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>is upset that he can't update his Facebook by ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>@Kenichan I dived many times for the ball. Man...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>my whole body feels itchy and like its on fire</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>@nationwideclass no, it's not behaving at all....</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# importing data\n",
    "columns = ['target', 'text']\n",
    "df_data = pd.read_csv(csv_name, usecols = [0, 5], header = None, names = columns)\n",
    "print(f'df_data dimensions: {df_data.shape}')\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "df_repl dimensions: (138, 2)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  emoticon description\n",
       "0      :?)       smile\n",
       "1       :)       smile\n",
       "2      :-]       smile\n",
       "3       :]       smile\n",
       "4      :-3       smile"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>emoticon</th>\n      <th>description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>:?)</td>\n      <td>smile</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>:)</td>\n      <td>smile</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>:-]</td>\n      <td>smile</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>:]</td>\n      <td>smile</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>:-3</td>\n      <td>smile</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# importing emoticon descriptions\n",
    "df_repl = pd.read_csv('emoticon_descriptions.csv', header = 0, usecols =[0, 1], names = ['emoticon', 'description'])\n",
    "print(f'df_repl dimensions: {df_repl.shape}')\n",
    "df_repl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving to dictionary\n",
    "dict_emot = {a:b for a, b in zip(df_repl.iloc[:, 0], df_repl.iloc[:, 1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding html replacements\n",
    "dict_emot['&quot;'] = 'quote'\n",
    "dict_emot['&amp;'] = 'and'\n",
    "dict_emot['&lt;'] = 'less than'\n",
    "dict_emot['&gt;'] = 'greater than'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dictionary of common abbreviations\n",
    "df_repl = pd.read_csv('common_abbreviations.csv').applymap(lambda x: x.lower())\n",
    "dict_abbr = {a:b for a, b in zip(df_repl.iloc[:, 0], df_repl.iloc[:, 1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general preprocessing tweet body text\n",
    "def general_preprocess(text):\n",
    "    '''\n",
    "    Returns a generally-applicable preprocessed version of the passed string.\n",
    "\n",
    "        Parameters:\n",
    "            text (str) : passed string\n",
    "        \n",
    "        Returns:\n",
    "            mod_text (str) : preprocessed string\n",
    "    '''\n",
    "\n",
    "    # add leading and trailing whitespace\n",
    "    mod_text = ' ' + text + ' '\n",
    "\n",
    "    # replace usernames\n",
    "    mod_text = re.sub(r'(?<=\\s)(@\\S+)(?=\\s)', ' USER ', mod_text)\n",
    "    \n",
    "    # replace urls\n",
    "    mod_text = re.sub(r'(?<=\\s)(https?:\\/\\/\\S+)(?=\\s)', ' URL ', mod_text)\n",
    "    \n",
    "    # replace non-space whitespace\n",
    "    mod_text = re.sub(r'\\s+', ' ', mod_text)\n",
    "    \n",
    "    # replace emoticons with text\n",
    "    for i, k in dict_emot.items():\n",
    "        mod_text = mod_text.replace(i, f' {k} ')\n",
    "    \n",
    "    # replace unrecognized characters\n",
    "    mod_text = mod_text.replace('İ', 'I')\n",
    "    \n",
    "    # expand contractions\n",
    "    mod_text = contractions.fix(mod_text)\n",
    "\n",
    "    # remove non-alphanumeric characters\n",
    "    mod_text = re.sub(r'[^a-zA-Z0-9]', ' ', mod_text)\n",
    "\n",
    "    # lower case text\n",
    "    mod_text = mod_text.lower()\n",
    "\n",
    "    # truncate repeated characters\n",
    "    mod_text = re.sub(r'(.)\\1{2,}', r'\\1\\1', mod_text)\n",
    "\n",
    "    # replace common abbreviations\n",
    "    for i, k in dict_abbr.items():\n",
    "        mod_text = mod_text.replace(f' {i} ', f' {k} ')\n",
    "    \n",
    "    # remove repeated spaces\n",
    "    mod_text = re.sub(r'( )\\1+', ' ', mod_text)\n",
    "\n",
    "    # trim leading / trailing whitespace\n",
    "    mod_text = mod_text.strip()\n",
    "\n",
    "    return mod_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   target                                               text\n",
       "0       0  user url aww that is a bummer you shoulda got ...\n",
       "1       0  is upset that he can not update his facebook b...\n",
       "2       0  user i dived many times for the ball managed t...\n",
       "3       0     my whole body feels itchy and like its on fire\n",
       "4       0  user no it is not behaving at all i am mad why..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>user url aww that is a bummer you shoulda got ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>is upset that he can not update his facebook b...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>user i dived many times for the ball managed t...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>my whole body feels itchy and like its on fire</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>user no it is not behaving at all i am mad why...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# apply function to dataframe\n",
    "df_data.loc[:, 'text'] = df_data['text'].apply(general_preprocess)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving dataframe to pickle\n",
    "df_data.to_pickle('df_data.pkl')"
   ]
  },
  {
   "source": [
    "## Single Layer Machine Learning Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### More Processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The most basic version of natural language encoding identifies documents by unique sets of word tokens and counts. The ML models in this section will all be trained using an optimized variant of this method, term frequency-inverse document frequency vectors. TF-IDF weights the word token counts to highlight words that are more important to a document, while vectorization converts the string of text into a more machine-comprehensible vector of numbers.\n",
    "\n",
    "Two constraints will be used to futher filter and simplify our corpus dictionary: removing stopwords, and lemmatization. Both techniques will result in a loss of contextual information, hence their absence from the prior section, though this is a necessary tradeoff to keep our vector space small."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/parkernisbet/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /home/parkernisbet/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# downloading stopwords and lemmas from nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('wordnet')\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional preprocessing for machine learning methods\n",
    "def sl_preprocess(text):\n",
    "    '''\n",
    "    Returns a preprocessed version of the passed string useful for single layer methods.\n",
    "    \n",
    "        Parameters:\n",
    "            text (str) : passed string\n",
    "        \n",
    "        Returns:\n",
    "            mod_text (str) : preprocessed string\n",
    "    '''\n",
    "    \n",
    "    # split string into tokens, removing stopwords and single characters, lemmatize\n",
    "    mod_text = ' '.join([lemma.lemmatize(i) for i in text.split() if len(i) > 1 if i not in stop_words])\n",
    "\n",
    "    return mod_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   target                                               text\n",
       "0       0  user url aww bummer shoulda got david carr thi...\n",
       "1       0  upset update facebook texting might cry result...\n",
       "2       0  user dived many time ball managed save 50 rest...\n",
       "3       0                    whole body feel itchy like fire\n",
       "4       0                              user behaving mad see"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>user url aww bummer shoulda got david carr thi...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>upset update facebook texting might cry result...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>user dived many time ball managed save 50 rest...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>whole body feel itchy like fire</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>user behaving mad see</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# applying function to dataframe\n",
    "df_data.loc[:, 'text'] = df_data['text'].apply(sl_preprocess)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving to pickle\n",
    "df_data.to_pickle('df_data_sl.pkl')"
   ]
  },
  {
   "source": [
    "# test train split\n",
    "sss = StratifiedShuffleSplit(n_splits = 1, test_size = .2, random_state = 3)\n",
    "sss.get_n_splits(df_data.text, df_data.target)\n",
    "for train_ind, test_ind in sss.split(df_data.text, df_data.target):\n",
    "    pass\n",
    "print(f'Train_ind shape: {train_ind.shape}\\nTest_ind shape: {test_ind.shape}')\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train_ind shape: (1280000,)\nTest_ind shape: (320000,)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf vectorizing\n",
    "vectorizer = TfidfVectorizer(analyzer = 'word', max_features = 300000)\n",
    "X_train = vectorizer.fit_transform(df_data.text[train_ind])\n",
    "y_train = df_data.target[train_ind]\n",
    "X_test = vectorizer.fit_transform(df_data.text[test_ind])\n",
    "y_test = df_data.target[test_ind]"
   ]
  },
  {
   "source": [
    "### Model Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}