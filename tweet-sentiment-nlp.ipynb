{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0039ba0ea65d4ae83123fad7f1d7f3994334a912b8446d9a8b7ab4243a3317da3",
   "display_name": "Python 3.8.8 64-bit ('tweet-sentiment-nlp': virtualenvwrapper)"
  },
  "metadata": {
   "interpreter": {
    "hash": "039ba0ea65d4ae83123fad7f1d7f3994334a912b8446d9a8b7ab4243a3317da3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Tweet Sentiment NLP"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This notebook covers sentiment analysis for a collection of 1.6 million tweets, sourced from this Kaggle [dataset](https://www.kaggle.com/kazanova/sentiment140). My goal is build a series of machine learning models that can predict document polarity reasonably accurately, with each attempt attacking the classification problem from a different natural language processing approach and level of sophistication. The below code can be split into three sections: generalized text preprocessing, single-layer model evaluation, and multi-layer model evaluation. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Generalized Text Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Basic Corpus Cleaning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "For the generalized text preprocessing section, modifications made are only those that do not intentionally \"lose\" tweet information (i.e. no dropping words or ther destructive changes). This then leaves only formatting, standardization, and other methods of purely data cleaning. See the \"general_preprocessing\" function for a full list and order of all data preparation techniques used. A general summary of changes made is as follows: replacing urls, usernames, emoticons, and unrecognized characters with words; expanding contractions and common abbreviations; removing all non-alphanumeric characters; and truncating egregiously long sequences of repeated characters. This should then leave the corpus comprised solely of recognized English language words and numbers."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if modules are installed\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "output = Popen(\"pip list | awk '{print $1}'\", shell = True, stdout=PIPE).stdout.read().split()\n",
    "packages = [x.decode('utf-8') for x in output][2:]\n",
    "modules = ['bayesian-optimization', 'contractions', 'kaggle', 'nltk', 'pandas', 'seaborn', 'scikit-learn']\n",
    "for nm in modules:\n",
    "    if nm not in packages:\n",
    "        ! pip install {nm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules and setup\n",
    "import contractions\n",
    "import csv\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import zipfile\n",
    "from bayes_opt import BayesianOptimization\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Skipping because df_data.pkl exists.\n"
     ]
    }
   ],
   "source": [
    "# downloading dataset\n",
    "if not os.path.isfile('df_data.pkl'):\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    dataset = 'kazanova/sentiment140'\n",
    "    csv_name = 'training.1600000.processed.noemoticon.csv'\n",
    "    try:\n",
    "        os.remove(csv_name)\n",
    "    except:\n",
    "        pass\n",
    "    api.dataset_download_file(dataset, file_name=csv_name, path='./')\n",
    "    fn = glob.glob('train*.zip', recursive = True)[0]\n",
    "    with zipfile.ZipFile(fn) as zip_file:\n",
    "        for file in zip_file.namelist():\n",
    "            if file == csv_name:\n",
    "                zip_file.extract(csv_name)\n",
    "    os.remove(fn)\n",
    "else:\n",
    "    print('Skipping because df_data.pkl exists.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Skipping because df_data.pkl exists.\n"
     ]
    }
   ],
   "source": [
    "# importing data\n",
    "if not os.path.isfile('df_data.pkl'):\n",
    "    columns = ['target', 'text']\n",
    "    df_data = pd.read_csv(csv_name, usecols = [0, 5], header = None, names = columns)\n",
    "    print(f'df_data dimensions: {df_data.shape}')\n",
    "    df_data.head()\n",
    "else:\n",
    "    print('Skipping because df_data.pkl exists.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Skipping because df_data.pkl exists.\n"
     ]
    }
   ],
   "source": [
    "# importing emoticon descriptions\n",
    "if not os.path.isfile('df_data.pkl'):\n",
    "    df_repl = pd.read_csv('emoticon_descriptions.csv', header = 0, usecols =[0, 1], \\\n",
    "        names = ['emoticon', 'description'])\n",
    "    print(f'df_repl dimensions: {df_repl.shape}')\n",
    "    df_repl.head()\n",
    "else:\n",
    "    print('Skipping because df_data.pkl exists.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Skipping because df_data.pkl exists.\n"
     ]
    }
   ],
   "source": [
    "# moving to dictionary\n",
    "if not os.path.isfile('df_data.pkl'):\n",
    "    dict_emot = {a:b for a, b in zip(df_repl.iloc[:, 0], df_repl.iloc[:, 1])}\n",
    "else:\n",
    "    print('Skipping because df_data.pkl exists.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Skipping because df_data.pkl exists.\n"
     ]
    }
   ],
   "source": [
    "# adding html replacements\n",
    "if not os.path.isfile('df_data.pkl'):\n",
    "    dict_emot['&quot;'] = 'quote'\n",
    "    dict_emot['&amp;'] = 'and'\n",
    "    dict_emot['&lt;'] = 'less than'\n",
    "    dict_emot['&gt;'] = 'greater than'\n",
    "else:\n",
    "    print('Skipping because df_data.pkl exists.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Skipping because df_data.pkl exists.\n"
     ]
    }
   ],
   "source": [
    "# creating dictionary of common abbreviations\n",
    "if not os.path.isfile('df_data.pkl'):\n",
    "    df_repl = pd.read_csv('common_abbreviations.csv').applymap(lambda x: x.lower())\n",
    "    dict_abbr = {a:b for a, b in zip(df_repl.iloc[:, 0], df_repl.iloc[:, 1])}\n",
    "else:\n",
    "    print('Skipping because df_data.pkl exists.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general preprocessing tweet body text\n",
    "def general_preprocess(text):\n",
    "    '''\n",
    "    Returns a generally-applicable preprocessed version of the passed string.\n",
    "\n",
    "        Parameters:\n",
    "            text (str) : passed string\n",
    "        \n",
    "        Returns:\n",
    "            mod_text (str) : preprocessed string\n",
    "    '''\n",
    "\n",
    "    # add leading and trailing whitespace\n",
    "    mod_text = ' ' + text + ' '\n",
    "\n",
    "    # replace usernames\n",
    "    mod_text = re.sub(r'(?<=\\s)(@\\S+)(?=\\s)', ' USER ', mod_text)\n",
    "    \n",
    "    # replace urls\n",
    "    mod_text = re.sub(r'(?<=\\s)(https?:\\/\\/\\S+)(?=\\s)', ' URL ', mod_text)\n",
    "    \n",
    "    # replace non-space whitespace\n",
    "    mod_text = re.sub(r'\\s+', ' ', mod_text)\n",
    "    \n",
    "    # replace emoticons with text\n",
    "    for i, k in dict_emot.items():\n",
    "        mod_text = mod_text.replace(i, f' {k} ')\n",
    "    \n",
    "    # replace unrecognized characters\n",
    "    mod_text = mod_text.replace('Ä°', 'I')\n",
    "    \n",
    "    # expand contractions\n",
    "    mod_text = contractions.fix(mod_text)\n",
    "\n",
    "    # remove non-alphanumeric characters\n",
    "    mod_text = re.sub(r'[^a-zA-Z0-9]', ' ', mod_text)\n",
    "\n",
    "    # lower case text\n",
    "    mod_text = mod_text.lower()\n",
    "\n",
    "    # truncate repeated characters\n",
    "    mod_text = re.sub(r'(.)\\1{2,}', r'\\1\\1', mod_text)\n",
    "\n",
    "    # replace common abbreviations\n",
    "    for i, k in dict_abbr.items():\n",
    "        mod_text = mod_text.replace(f' {i} ', f' {k} ')\n",
    "    \n",
    "    # remove repeated spaces\n",
    "    mod_text = re.sub(r'( )\\1+', ' ', mod_text)\n",
    "\n",
    "    # trim leading / trailing whitespace\n",
    "    mod_text = mod_text.strip()\n",
    "\n",
    "    return mod_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Skipping because df_data.pkl exists.\n"
     ]
    }
   ],
   "source": [
    "# apply function to dataframe\n",
    "if not os.path.isfile('df_data.pkl'):\n",
    "    df_data.loc[:, 'text'] = df_data['text'].apply(general_preprocess)\n",
    "    df_data.head()\n",
    "else:\n",
    "    print('Skipping because df_data.pkl exists.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Skipping because df_data.pkl exists.\n"
     ]
    }
   ],
   "source": [
    "# doing some pickling\n",
    "if not os.path.isfile('df_data.pkl'):\n",
    "    df_data.to_pickle('df_data.pkl')\n",
    "else:\n",
    "    print('Skipping because df_data.pkl exists.')"
   ]
  },
  {
   "source": [
    "## Single Layer Machine Learning Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### More Processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The most basic version of natural language encoding identifies documents by unique sets of word tokens and counts. The ML models in this section will all be trained using an optimized variant of this method, term frequency-inverse document frequency vectors. TF-IDF weights the word token counts to highlight words that are more important to a document, while vectorization converts the string of text into a more machine-comprehensible vector of numbers.\n",
    "\n",
    "Two constraints will be used to futher filter and simplify our corpus dictionary: removing stopwords, and lemmatization. Both techniques will result in a loss of contextual information, hence their absence from the prior section, though this is a necessary tradeoff to keep our vector space small."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/parkernisbet/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /home/parkernisbet/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# downloading stopwords and lemmas from nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download('wordnet')\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional preprocessing for machine learning methods\n",
    "def sl_preprocess(text):\n",
    "    '''\n",
    "    Returns a preprocessed version of the passed string useful for single layer methods.\n",
    "    \n",
    "        Parameters:\n",
    "            text (str) : passed string\n",
    "        \n",
    "        Returns:\n",
    "            mod_text (str) : preprocessed string\n",
    "    '''\n",
    "    \n",
    "    # split string into tokens, removing stopwords and single characters, lemmatize\n",
    "    mod_text = ' '.join([lemma.lemmatize(i) for i in text.split() if len(i) > 1 if i not in stop_words])\n",
    "\n",
    "    return mod_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Skipping because df_data_sl.pkl exists.\n"
     ]
    }
   ],
   "source": [
    "# applying function to dataframe\n",
    "if not os.path.isfile('df_data_sl.pkl'):\n",
    "    df_data.loc[:, 'text'] = df_data['text'].apply(sl_preprocess)\n",
    "    df_data.head()\n",
    "else:\n",
    "    print('Skipping because df_data_sl.pkl exists.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "df_data_sl.pkl read into df_data.\n"
     ]
    }
   ],
   "source": [
    "# saving to pickle\n",
    "if not os.path.isfile('df_data_sl.pkl'):\n",
    "    df_data.to_pickle('df_data_sl.pkl')\n",
    "else:\n",
    "    df_data = pd.read_pickle('df_data_sl.pkl')\n",
    "    print(\"df_data_sl.pkl read into df_data.\")"
   ]
  },
  {
   "source": [
    "# test train split\n",
    "sss = StratifiedShuffleSplit(n_splits = 1, test_size = .2, random_state = 3)\n",
    "sss.get_n_splits(df_data.text, df_data.target)\n",
    "for train_ind, test_ind in sss.split(df_data.text, df_data.target):\n",
    "    pass\n",
    "print(f'Train_ind shape: {train_ind.shape}\\nTest_ind shape: {test_ind.shape}')\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train_ind shape: (1280000,)\nTest_ind shape: (320000,)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf vectorizing\n",
    "vectorizer = TfidfVectorizer(analyzer = 'word', max_features = 300000)\n",
    "X_train = vectorizer.fit_transform(df_data.text[train_ind])\n",
    "y_train = df_data.target[train_ind]\n",
    "X_test = vectorizer.transform(df_data.text[test_ind])\n",
    "y_test = df_data.target[test_ind]"
   ]
  },
  {
   "source": [
    "### Model Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The vectorizer object above converts the dataframe of strings into a sparse matrix, so this slightly limits which sklearn models we can use for prediction. A detailed list of compatible methods can be found [here](https://dziganto.github.io/Sparse-Matrices-For-Efficient-Machine-Learning/), however this notebook will only cover a small subset of sklearn's classifiers:\n",
    "\n",
    "    - LogisticRegression\n",
    "    - SVC\n",
    "    - MultinomialNB\n",
    "    - DecisionTreeClassifier\n",
    "    - RandomForestClassifier\n",
    "    - AdaBoostClassifier\n",
    "    - MLPClassifier\n",
    "\n",
    "While the order the above classifiers are solved in does not matter, it is worth noting that the last three (RandomForestClassifier and down) are ensemble classifiers."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for storing model performance\n",
    "model_perf = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression function for hyperparameter tuning\n",
    "def log_func(C = 1, do_more = 0):\n",
    "    '''\n",
    "    Function to optimize via Bayesian optimization.and\n",
    "\n",
    "        Parameters:\n",
    "            C (float) : inverse regularization strength\n",
    "            do_more (integer) : flag for function to store model performance information in model_perf\n",
    "        \n",
    "        Returns:\n",
    "            acc (float) : prediction accuracy\n",
    "    '''\n",
    "\n",
    "    global model_perf\n",
    "    model = LogisticRegression(C = C, max_iter = 1000, random_state = 4).fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = 100*np.sum(y_pred == y_test)/y_test.size\n",
    "    if do_more == 1:\n",
    "        c_mat = confusion_matrix(y_test, t_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        model_perf['log'] = (acc, c_mat, f1)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding our optimal logistic regression C value\n",
    "bounds = {'C': (.001, 1000)}\n",
    "opt = BayesianOptimization(f = log_func, pbounds = bounds, random_state = 4)\n",
    "opt.maximize(init_points = 10, n_iter = 20)\n",
    "print(opt.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the model perf\n",
    "C = optimizer.max['params']['C']\n",
    "print(f'Logistic Regression accuracy: {round(log_func(C = C, do_more = 1), 2)}')"
   ]
  }
 ]
}